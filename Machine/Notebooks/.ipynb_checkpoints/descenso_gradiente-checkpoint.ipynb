{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e3a0cd",
   "metadata": {},
   "source": [
    "# Regresión lineal usando descenso por gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d17423e",
   "metadata": {},
   "source": [
    "En esta libreta programaremos y evaluaremos el algoritmo de descenso por gradiente para regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83014e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as ecm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf82b4",
   "metadata": {},
   "source": [
    "## Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f0777",
   "metadata": {},
   "source": [
    "Posteriormente cargamos los datos de GPAs de alumnos de licenciatura (fuente: http://onlinestatbook.com/2/case_studies/sat.html). Usamos sólo el atributo Computer Science GPA (columna <tt>comp_GPA</tt>) como regresor para el University GPA (columna <tt>univ_GPA</tt>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64ec6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = pd.read_csv('http://onlinestatbook.com/2/case_studies/data/sat.txt', sep=' ')\n",
    "\n",
    "X = sat.comp_GPA.to_numpy()[:, np.newaxis]\n",
    "y = sat.univ_GPA.to_numpy()[:, np.newaxis]\n",
    "\n",
    "ones = np.ones((X.shape[0], 1)) # vector columna de unos\n",
    "X = np.concatenate([ones, X], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9511ac",
   "metadata": {},
   "source": [
    "Dividimos aleatoriamente el conjunto de datos en 80% para entrenamiento y 20% para validación. Para eso usaremos la función <tt>train_test_split</tt> de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77271e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ent, X_valid, y_ent, y_valid = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "X_rango = np.arange(2.0, 4.0, 0.01)[:, np.newaxis]\n",
    "ones_rango = np.ones((X_rango.shape[0], 1))\n",
    "X_rango = np.concatenate((ones_rango, X_rango), axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4304d",
   "metadata": {},
   "source": [
    "## Descenso por gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39414b38",
   "metadata": {},
   "source": [
    "Vamos a entrenar un modelo de regresión lineal usando descenso por gradiente. En particular, buscaremos minimizar la suma de errores cuadráticos, la cual definimos de la siguiente manera: $$ E(y, \\hat{y}) = \\frac{1}{2} \\sum_{i=1}^{n}(\\hat{y}^{(i)} - y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cf6af",
   "metadata": {},
   "source": [
    "El gradiente de esta función de pérdida respecto a los parámetros $\\theta \\in \\mathcal{R}^d$ está dado por\n",
    "$$ \\nabla E(y, \\hat{y}) = [\\frac{\\partial E(y, \\hat{y})}{\\partial \\theta_0}, \\frac{\\partial E(y, \\hat{y})}{\\partial \\theta_1}, ..., \\frac{\\partial E(y, \\hat{y})}{\\partial \\theta_d}]$$\n",
    "donde\n",
    "$$ \\frac{\\partial E(y, \\hat{y})}{\\partial \\theta_j} = \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) \\cdot x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b868cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(X, y, y_pred):\n",
    "    return X.T @ (y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5dec9",
   "metadata": {},
   "source": [
    "El algoritmo de descenso por gradiente quedaría como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_gradiente(X, y, n_iter = 10, tasa = 0.001):\n",
    "    hist_error = np.zeros(n_iter)\n",
    "    # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
