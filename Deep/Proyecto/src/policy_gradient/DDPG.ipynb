{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Deterministic Policy Gradient\n",
        "\n",
        "En este notebook se implementará el algoritmo DDPG y se aplicará a un ambiente de gymnasium."
      ],
      "metadata": {
        "id": "8-R7FEt9abkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se importarán las bibliotecas necesarias."
      ],
      "metadata": {
        "id": "DYJQN_UsbvMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import gymnasium as gym\n",
        "except:\n",
        "    !pip install gymnasium[mujoco]\n",
        "    import gymnasium as gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zta2RTC6cwML",
        "outputId": "a458e74c-d260-4a2e-b26d-c94d5459d6a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[mujoco]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[mujoco])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.31.6)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.5.2)\n",
            "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
            "  Downloading glfw-2.6.3-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.17.0)\n",
            "Installing collected packages: glfw, farama-notifications, gymnasium, mujoco\n",
            "Successfully installed farama-notifications-0.0.4 glfw-2.6.3 gymnasium-0.29.1 mujoco-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import ptan\n",
        "except:\n",
        "    !pip install ptan\n",
        "    import ptan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJZ2Bslx97Aq",
        "outputId": "91838a16-38ed-4eed-e4cf-87dded9f1cdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptan\n",
            "  Downloading ptan-0.7.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of ptan to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ptan-0.6.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading ptan-0.4.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ptan\n",
            "  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptan: filename=ptan-0.4-py3-none-any.whl size=21512 sha256=8c619b628850d712f2afaad37ea69f9d0252dc203d9cfa74357e175db0f7db20\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/4e/eb/5a6f69a1f1b375dcec4ee8f623856740b417fdcacadd72e302\n",
            "Successfully built ptan\n",
            "Installing collected packages: ptan\n",
            "Successfully installed ptan-0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "except:\n",
        "    !pip install tensorboardX\n",
        "    from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae-x_WskLzku",
        "outputId": "e79da868-75e9-4de3-bc3e-4f84cda06978"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jl_qsJAab0Y5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La biblioteca PTAN\n",
        "\n",
        "Para simplificar la implementación, se usará la biblioteca PTAN (Pytorch AgentNet), la cual es una caja de herramientas de aprendizaje por refuerzo.\n",
        "\n",
        "Para esta implementación se usará la plantilla para el agente `BaseAgent`. Para el buffer de recuerdos se usará la clase `ExperienceReplayBuffer`."
      ],
      "metadata": {
        "id": "laBnPE1Eg716"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El actor\n",
        "\n",
        "Se creará la clase para la red neuronal relacionada con la política, a la cual se le llama *actor*."
      ],
      "metadata": {
        "id": "aZ1sPjmrdmZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_size, act_size):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.red = nn.Sequential(\n",
        "            nn.BatchNorm1d(obs_size),\n",
        "            nn.Linear(obs_size, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, act_size),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # en el caso de cartpole de Mujoco, la señal está en el rango [-3,3]\n",
        "        # por eso se multiplica el resultado de tanh por 3\n",
        "        return 3*self.net(x)"
      ],
      "metadata": {
        "id": "onybJHIheYNH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El crítico\n",
        "\n",
        "Ahora se creará la clase para la red neuronal relacionada con la función de acción-valor ($Q(s, a)$).\n",
        "\n",
        "Las acciones no se incluyen hasta la segunda capa de $Q$."
      ],
      "metadata": {
        "id": "3TzW6hQf6oak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_size, act_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.obs_net = nn.Sequential(\n",
        "            nn.BatchNorm1d(obs_size),\n",
        "            nn.Linear(obs_size, 400),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.out_net = nn.Sequential(\n",
        "            nn.Linear(400 + act_size, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 1))\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        obs = self.obs_net(x)\n",
        "        return self.out_net(torch.cat([obs, a], dim=1))"
      ],
      "metadata": {
        "id": "yf6m06Lf64lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbce64cd-ceb7-40e1-d34d-ff649658532c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El agente\n",
        "\n",
        "Ahora se creará la clase para el agente de aprendizaje por refuerzo.\n",
        "\n",
        "Para la exploración se usa ruido correlacionado. Se usa el proceso de *Ornstein-Uhlenbeck* con $\\theta = 0.15$ y $\\sigma = 0.2$, con una media de 0."
      ],
      "metadata": {
        "id": "Ciej-NBx-J8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(ptan.agent.BaseAgent):\n",
        "    def __init__(self, net, device='cpu', ou_enabled=True, ou_mu=0.0, ou_teta=0.15, ou_sigma=0.2, ou_epsilon=1.0):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.ou_enabled = ou_enabled\n",
        "        self.ou_mu = ou_mu\n",
        "        self.ou_teta = ou_teta\n",
        "        self.ou_sigma = ou_sigma\n",
        "        self.ou_epsilon = ou_epsilon\n",
        "\n",
        "    def initial_state(self):\n",
        "        return None\n",
        "\n",
        "    def __call__(self, states, agent_states):\n",
        "        states_v = ptan.agent.float32_preprocessor(states)\n",
        "        states_v = states_v.to(self.device)\n",
        "        mu_v = self.net(states_v)\n",
        "        actions = mu_v.data.cpu().numpy()\n",
        "\n",
        "        if self.ou_enabled and self.ou_epsilon > 0:\n",
        "            new_a_states = []\n",
        "            for a_state, action in zip(agent_states, actions):\n",
        "                if a_state is None:\n",
        "                    a_state = np.zeros(shape=action.shape, dtype=np.float32)\n",
        "                a_state += self.ou_teta*(self.ou_mu - a_state)\n",
        "                a_state += self.ou_sigma*np.random.normal(size=action.shape)\n",
        "                action += self.ou_epsilon*a_state\n",
        "                new_a_states.append(a_state)\n",
        "        else:\n",
        "            new_a_states = agent_states\n",
        "        actions = np.clip(actions, -1, 1)\n",
        "        return actions, new_a_states"
      ],
      "metadata": {
        "id": "EWAvFae2-QUT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento del lote\n",
        "\n",
        "Se usará una función para desempacar el lote."
      ],
      "metadata": {
        "id": "Y2WBsgcr44kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_batch(batch, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
        "    for exp in batch:\n",
        "        states.append(exp.state)\n",
        "        actions.append(exp.action)\n",
        "        rewards.append(exp.reward)\n",
        "        dones.append(exp.last_state is None)\n",
        "        if exp.last_state is None:\n",
        "            last_states.append(exp.state)\n",
        "        else:\n",
        "            last_states.append(exp.last_state)\n",
        "    states_v = ptan.agent.float32_preprocessor(states).to(device)\n",
        "    actions_v = ptan.agent.float32_preprocessor(actions).to(device)\n",
        "    rewards_v = ptan.agent.float32_preprocessor(rewards).to(device)\n",
        "    last_states_v = ptan.agent.float32_preprocessor(last_states).to(device)\n",
        "    dones_t = torch.BoolTensor(dones).to(device)\n",
        "    return states_v, actions_v, rewards_v, dones_t, last_states_v"
      ],
      "metadata": {
        "id": "55_qEI6T5FGe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fase de entrenamiento\n",
        "\n",
        "Primero se definirán los parámetros para la fase de entrenamiento."
      ],
      "metadata": {
        "id": "rfLZiGjiDCnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_ID = 'InvertedPendulum-v4' # cartpole o péndulo invertido\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE_ACTOR = 1e-4\n",
        "LEARNING_RATE_CRITIC = 1e-3\n",
        "REPLAY_SIZE = 10000\n",
        "REPLAY_INITIAL = 1000\n",
        "TEST_ITERS = 50\n",
        "MAX_EPOCAS = 2000 # cambiar dependiendo del tiempo que tarda"
      ],
      "metadata": {
        "id": "nWlQstDPDJbu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_net(net, env, count=10, device='cpu'):\n",
        "    rewards = 0.0\n",
        "    steps = 0\n",
        "    for _ in range(count):\n",
        "        obs = env.reset()\n",
        "        terminado = False\n",
        "        while not terminado:\n",
        "            obs_v = ptan.agent.float32_preprocessor([obs]).to(device)\n",
        "            mu_v = net(obs_v)\n",
        "            action = mu_v.squeeze(dim=0).data.cpu().numpy()\n",
        "            action = np.clip(action, -3, 3) # acotar la acción entre -3 y 3\n",
        "            obs, reward, done, truncated, _ = env.step(action)\n",
        "            rewards += reward\n",
        "            steps += 1\n",
        "            if done or truncated:\n",
        "                terminado = True\n",
        "    return rewards/count, steps/count"
      ],
      "metadata": {
        "id": "JzYBkF9jVPNS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entrenamiento():\n",
        "    recompensas = []\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    save_path = os.path.join(\"saves\", \"ddpg-\" + 'cartpole')\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    env = gym.make(ENV_ID)\n",
        "    test_env = gym.make(ENV_ID)\n",
        "\n",
        "    act_net = Actor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
        "    crt_net = Critic(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
        "    # Target networks\n",
        "    tgt_act_net = ptan.agent.TargetNet(act_net)\n",
        "    tgt_crt_net = ptan.agent.TargetNet(crt_net)\n",
        "\n",
        "    writer = SummaryWriter(comment='-ddpg_'+'cartpole')\n",
        "    agent = Agent(act_net, device = device)\n",
        "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=1)\n",
        "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
        "    act_opt = optim.Adam(act_net.parameters(), lr=LEARNING_RATE_ACTOR)\n",
        "    crt_opt = optim.Adam(crt_net.parameters(), lr=LEARNING_RATE_CRITIC)\n",
        "\n",
        "    frame_idx = 0\n",
        "    best_reward = None\n",
        "    with ptan.common.utils.RewardTracker(writer) as tracker:\n",
        "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
        "            for _ in range(MAX_EPOCAS):\n",
        "                frame_idx += 1\n",
        "                buffer.populate(1)\n",
        "                rewards_steps = exp_source.pop_rewards_steps()\n",
        "                if rewards_steps:\n",
        "                    rewards, steps = zip(*rewards_steps)\n",
        "                    tb_tracker.track('episode_steps', steps[0], frame_idx)\n",
        "                    tracker.reward(rewards[0], frame_idx)\n",
        "\n",
        "                if len(buffer) < REPLAY_INITIAL:\n",
        "                    continue\n",
        "\n",
        "                batch = buffer.sample(BATCH_SIZE)\n",
        "                states_v, actions_v, rewards_v, dones_mask, last_states_v = unpack_batch(batch, device)\n",
        "\n",
        "                # train critic\n",
        "                crt_opt.zero_grad()\n",
        "                q_v = crt_net(states_v, actions_v)\n",
        "                last_act_v = tgt_act_net.target_model(last_states_v, last_act_v)\n",
        "                q_last_v = tgt_crt_net.target_model(last_states_v, last_act_v)\n",
        "                q_last_v[dones_mask] = 0.0\n",
        "                q_ref_v = rewards_v.unsqueeze(dim=-1) + q_last_v * GAMMA\n",
        "                critic_loss_v = F.mse_loss(q_v, q_ref_v.detach())\n",
        "                critic_loss_v.backward()\n",
        "                crt_opt.step()\n",
        "                tb_tracker.track(\"loss_critic\", critic_loss_v, frame_idx)\n",
        "                tb_tracker.track(\"critic_ref\", q_ref_v.mean(), frame_idx)\n",
        "\n",
        "                # train actor\n",
        "                act_opt.zero_grad()\n",
        "                cur_actions_v = act_net(states_v)\n",
        "                actor_loss_v = -crt_net(states_v, cur_actions_v)\n",
        "                actor_loss_v = actor_loss_v.mean()\n",
        "                actor_loss_v.backward()\n",
        "                act_opt.step()\n",
        "                tb_tracker.track(\"loss_actor\", actor_loss_v, frame_idx)\n",
        "                tgt_act_net.alpha_sync(alpha=1 - 1e-3)\n",
        "                tgt_crt_net.alpha_sync(alpha=1 - 1e-3)\n",
        "\n",
        "                if frame_idx % TEST_ITERS == 0:\n",
        "                    ts = time.time()\n",
        "                    rewards, steps = test_net(act_net, test_env, device = device)\n",
        "                    print('Test done in %.2f sec, reward %.3f, steps %d' % (time.time()-ts, rewards, steps))\n",
        "                    writer.add_scalar('test_reward', rewards, frame_idx)\n",
        "                    writer.add_scalar('test_steps', steps, frame_idx)\n",
        "                    if best_reward is None or best_reward < rewards:\n",
        "                        if best_reward is not None:\n",
        "                            print('Best reward updated: %.3f -> %.3f' % (best_reward, rewards))\n",
        "                            name = 'best_%+.3f_%d.dat' % (rewards, frame_idx)\n",
        "                            fname = os.path.join(save_path, name)\n",
        "                            torch.save(act_net.state_dict(), fname)\n",
        "                        best_reward = rewards\n",
        "                    recompensas.append(best_reward)\n",
        "\n",
        "    # se guarda el último estado de la red actor\n",
        "    torch.save(act_net.state_dict(), 'actor_params.pt')\n",
        "    return recompensas\n"
      ],
      "metadata": {
        "id": "GaTAyV5BYj3g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte:\n",
        "- Se entrena el modelo.\n",
        "- Se obtienen las recompensas respecto a las épocas.\n",
        "- Se grafica la recompensa."
      ],
      "metadata": {
        "id": "q4mrXYRfGZMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recompensas_ent = entrenamiento()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "1doDICE0I9H4",
        "outputId": "ac737a9e-15ef-4ef4-ec71-d2363028bbee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-216c642e5ae7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecompensas_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-ace1e3618db6>\u001b[0m in \u001b[0;36mentrenamiento\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-ddpg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'cartpole'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mexp_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperienceReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREPLAY_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mact_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE_ACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, agent, gamma, steps_count, steps_delta, vectorized)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, agent, steps_count, steps_delta, vectorized)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvectorized\u001b[0m \u001b[0menvs\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenAI\u001b[0m \u001b[0muniverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(recompensas_ent)\n",
        "plt.xlabel('Cada', TEST_ITERS, 'épocas')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.legend('Gráfico')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GRitKYGsJKDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}