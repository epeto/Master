{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-R7FEt9abkZ"
   },
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "En este notebook se implementará el algoritmo DDPG y se aplicará a un ambiente de gymnasium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYJQN_UsbvMH"
   },
   "source": [
    "Primero se importarán las bibliotecas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zta2RTC6cwML",
    "outputId": "a458e74c-d260-4a2e-b26d-c94d5459d6a2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gymnasium as gym\n",
    "except:\n",
    "    !pip install gymnasium[mujoco]\n",
    "    import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJZ2Bslx97Aq",
    "outputId": "91838a16-38ed-4eed-e4cf-87dded9f1cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ptan\n",
      "  Downloading ptan-0.7.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting atari-py (from ptan)\n",
      "  Downloading atari-py-0.2.9.tar.gz (540 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.6/540.6 kB\u001b[0m \u001b[31m150.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gym (from ptan)\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m184.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from ptan) (1.21.5)\n",
      "Collecting opencv-python (from ptan)\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of ptan to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ptan\n",
      "  Downloading ptan-0.6.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading ptan-0.4.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ptan\n",
      "  Building wheel for ptan (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ptan: filename=ptan-0.4-py3-none-any.whl size=21532 sha256=64d9491a810c2ab5457d355adda0d96f91e37191c5800aa210ba84287202872a\n",
      "  Stored in directory: /home/emmanuel/.cache/pip/wheels/f9/4e/eb/5a6f69a1f1b375dcec4ee8f623856740b417fdcacadd72e302\n",
      "Successfully built ptan\n",
      "Installing collected packages: ptan\n",
      "Successfully installed ptan-0.4\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mptan\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ptan'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install ptan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mptan\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ptan/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m common\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m actions\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experience\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ptan/common/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m runfile\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers_simple\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ptan/common/wrappers.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import ptan\n",
    "except:\n",
    "    !pip install ptan\n",
    "    import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ae-x_WskLzku",
    "outputId": "e79da868-75e9-4de3-bc3e-4f84cda06978"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except:\n",
    "    !pip install tensorboardX\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jl_qsJAab0Y5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laBnPE1Eg716"
   },
   "source": [
    "## La biblioteca PTAN\n",
    "\n",
    "Para simplificar la implementación, se usará la biblioteca PTAN (Pytorch AgentNet), la cual es una caja de herramientas de aprendizaje por refuerzo.\n",
    "\n",
    "Para esta implementación se usará la plantilla para el agente `BaseAgent`. Para el buffer de recuerdos se usará la clase `ExperienceReplayBuffer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ1sPjmrdmZy"
   },
   "source": [
    "## El actor\n",
    "\n",
    "Se creará la clase para la red neuronal relacionada con la política, a la cual se le llama *actor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onybJHIheYNH"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.red = nn.Sequential(\n",
    "            nn.BatchNorm1d(obs_size),\n",
    "            nn.Linear(obs_size, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, act_size),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # en el caso de cartpole de Mujoco, la señal está en el rango [-3,3]\n",
    "        # por eso se multiplica el resultado de tanh por 3\n",
    "        return 3*self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TzW6hQf6oak"
   },
   "source": [
    "## El crítico\n",
    "\n",
    "Ahora se creará la clase para la red neuronal relacionada con la función de acción-valor ($Q(s, a)$).\n",
    "\n",
    "Las acciones no se incluyen hasta la segunda capa de $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yf6m06Lf64lw",
    "outputId": "cbce64cd-ceb7-40e1-d34d-ff649658532c"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.BatchNorm1d(obs_size),\n",
    "            nn.Linear(obs_size, 400),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(400 + act_size, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1))\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ciej-NBx-J8T"
   },
   "source": [
    "## El agente\n",
    "\n",
    "Ahora se creará la clase para el agente de aprendizaje por refuerzo.\n",
    "\n",
    "Para la exploración se usa ruido correlacionado. Se usa el proceso de *Ornstein-Uhlenbeck* con $\\theta = 0.15$ y $\\sigma = 0.2$, con una media de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWAvFae2-QUT"
   },
   "outputs": [],
   "source": [
    "class Agent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, net, device='cpu', ou_enabled=True, ou_mu=0.0, ou_teta=0.15, ou_sigma=0.2, ou_epsilon=1.0):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.ou_enabled = ou_enabled\n",
    "        self.ou_mu = ou_mu\n",
    "        self.ou_teta = ou_teta\n",
    "        self.ou_sigma = ou_sigma\n",
    "        self.ou_epsilon = ou_epsilon\n",
    "\n",
    "    def initial_state(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, states, agent_states):\n",
    "        states_v = ptan.agent.float32_preprocessor(states)\n",
    "        states_v = states_v.to(self.device)\n",
    "        mu_v = self.net(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "\n",
    "        if self.ou_enabled and self.ou_epsilon > 0:\n",
    "            new_a_states = []\n",
    "            for a_state, action in zip(agent_states, actions):\n",
    "                if a_state is None:\n",
    "                    a_state = np.zeros(shape=action.shape, dtype=np.float32)\n",
    "                a_state += self.ou_teta*(self.ou_mu - a_state)\n",
    "                a_state += self.ou_sigma*np.random.normal(size=action.shape)\n",
    "                action += self.ou_epsilon*a_state\n",
    "                new_a_states.append(a_state)\n",
    "        else:\n",
    "            new_a_states = agent_states\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions, new_a_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2WBsgcr44kA"
   },
   "source": [
    "## Preprocesamiento del lote\n",
    "\n",
    "Se usará una función para desempacar el lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55_qEI6T5FGe"
   },
   "outputs": [],
   "source": [
    "def unpack_batch(batch, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "    states_v = ptan.agent.float32_preprocessor(states).to(device)\n",
    "    actions_v = ptan.agent.float32_preprocessor(actions).to(device)\n",
    "    rewards_v = ptan.agent.float32_preprocessor(rewards).to(device)\n",
    "    last_states_v = ptan.agent.float32_preprocessor(last_states).to(device)\n",
    "    dones_t = torch.BoolTensor(dones).to(device)\n",
    "    return states_v, actions_v, rewards_v, dones_t, last_states_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfLZiGjiDCnD"
   },
   "source": [
    "## Fase de entrenamiento\n",
    "\n",
    "Primero se definirán los parámetros para la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWlQstDPDJbu"
   },
   "outputs": [],
   "source": [
    "ENV_ID = 'InvertedPendulum-v4' # cartpole o péndulo invertido\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE_ACTOR = 1e-4\n",
    "LEARNING_RATE_CRITIC = 1e-3\n",
    "REPLAY_SIZE = 10000\n",
    "REPLAY_INITIAL = 1000\n",
    "TEST_ITERS = 50\n",
    "MAX_EPOCAS = 2000 # cambiar dependiendo del tiempo que tarda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzYBkF9jVPNS"
   },
   "outputs": [],
   "source": [
    "def test_net(net, env, count=10, device='cpu'):\n",
    "    rewards = 0.0\n",
    "    steps = 0\n",
    "    for _ in range(count):\n",
    "        obs = env.reset()\n",
    "        terminado = False\n",
    "        while not terminado:\n",
    "            obs_v = ptan.agent.float32_preprocessor([obs]).to(device)\n",
    "            mu_v = net(obs_v)\n",
    "            action = mu_v.squeeze(dim=0).data.cpu().numpy()\n",
    "            action = np.clip(action, -3, 3) # acotar la acción entre -3 y 3\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "            if done or truncated:\n",
    "                terminado = True\n",
    "    return rewards/count, steps/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaTAyV5BYj3g"
   },
   "outputs": [],
   "source": [
    "def entrenamiento():\n",
    "    recompensas = []\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    save_path = os.path.join(\"saves\", \"ddpg-\" + 'cartpole')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    env = gym.make(ENV_ID)\n",
    "    test_env = gym.make(ENV_ID)\n",
    "\n",
    "    act_net = Actor(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "    crt_net = Critic(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "    # Target networks\n",
    "    tgt_act_net = ptan.agent.TargetNet(act_net)\n",
    "    tgt_crt_net = ptan.agent.TargetNet(crt_net)\n",
    "\n",
    "    writer = SummaryWriter(comment='-ddpg_'+'cartpole')\n",
    "    agent = Agent(act_net, device = device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=1)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
    "    act_opt = optim.Adam(act_net.parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "    crt_opt = optim.Adam(crt_net.parameters(), lr=LEARNING_RATE_CRITIC)\n",
    "\n",
    "    frame_idx = 0\n",
    "    best_reward = None\n",
    "    with ptan.common.utils.RewardTracker(writer) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            for _ in range(MAX_EPOCAS):\n",
    "                frame_idx += 1\n",
    "                buffer.populate(1)\n",
    "                rewards_steps = exp_source.pop_rewards_steps()\n",
    "                if rewards_steps:\n",
    "                    rewards, steps = zip(*rewards_steps)\n",
    "                    tb_tracker.track('episode_steps', steps[0], frame_idx)\n",
    "                    tracker.reward(rewards[0], frame_idx)\n",
    "\n",
    "                if len(buffer) < REPLAY_INITIAL:\n",
    "                    continue\n",
    "\n",
    "                batch = buffer.sample(BATCH_SIZE)\n",
    "                states_v, actions_v, rewards_v, dones_mask, last_states_v = unpack_batch(batch, device)\n",
    "\n",
    "                # train critic\n",
    "                crt_opt.zero_grad()\n",
    "                q_v = crt_net(states_v, actions_v)\n",
    "                last_act_v = tgt_act_net.target_model(last_states_v, last_act_v)\n",
    "                q_last_v = tgt_crt_net.target_model(last_states_v, last_act_v)\n",
    "                q_last_v[dones_mask] = 0.0\n",
    "                q_ref_v = rewards_v.unsqueeze(dim=-1) + q_last_v * GAMMA\n",
    "                critic_loss_v = F.mse_loss(q_v, q_ref_v.detach())\n",
    "                critic_loss_v.backward()\n",
    "                crt_opt.step()\n",
    "                tb_tracker.track(\"loss_critic\", critic_loss_v, frame_idx)\n",
    "                tb_tracker.track(\"critic_ref\", q_ref_v.mean(), frame_idx)\n",
    "\n",
    "                # train actor\n",
    "                act_opt.zero_grad()\n",
    "                cur_actions_v = act_net(states_v)\n",
    "                actor_loss_v = -crt_net(states_v, cur_actions_v)\n",
    "                actor_loss_v = actor_loss_v.mean()\n",
    "                actor_loss_v.backward()\n",
    "                act_opt.step()\n",
    "                tb_tracker.track(\"loss_actor\", actor_loss_v, frame_idx)\n",
    "                tgt_act_net.alpha_sync(alpha=1 - 1e-3)\n",
    "                tgt_crt_net.alpha_sync(alpha=1 - 1e-3)\n",
    "\n",
    "                if frame_idx % TEST_ITERS == 0:\n",
    "                    ts = time.time()\n",
    "                    rewards, steps = test_net(act_net, test_env, device = device)\n",
    "                    print('Test done in %.2f sec, reward %.3f, steps %d' % (time.time()-ts, rewards, steps))\n",
    "                    writer.add_scalar('test_reward', rewards, frame_idx)\n",
    "                    writer.add_scalar('test_steps', steps, frame_idx)\n",
    "                    if best_reward is None or best_reward < rewards:\n",
    "                        if best_reward is not None:\n",
    "                            print('Best reward updated: %.3f -> %.3f' % (best_reward, rewards))\n",
    "                            name = 'best_%+.3f_%d.dat' % (rewards, frame_idx)\n",
    "                            fname = os.path.join(save_path, name)\n",
    "                            torch.save(act_net.state_dict(), fname)\n",
    "                        best_reward = rewards\n",
    "                    recompensas.append(best_reward)\n",
    "\n",
    "    # se guarda el último estado de la red actor\n",
    "    torch.save(act_net.state_dict(), 'actor_params.pt')\n",
    "    return recompensas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4mrXYRfGZMh"
   },
   "source": [
    "En esta parte:\n",
    "- Se entrena el modelo.\n",
    "- Se obtienen las recompensas respecto a las épocas.\n",
    "- Se grafica la recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "1doDICE0I9H4",
    "outputId": "ac737a9e-15ef-4ef4-ec71-d2363028bbee"
   },
   "outputs": [],
   "source": [
    "recompensas_ent = entrenamiento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRitKYGsJKDI"
   },
   "outputs": [],
   "source": [
    "plt.plot(recompensas_ent)\n",
    "plt.xlabel('Cada', TEST_ITERS, 'épocas')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.legend('Gráfico')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
