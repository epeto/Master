{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da803ca",
   "metadata": {},
   "source": [
    "# Diferenciación automática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126accf",
   "metadata": {},
   "source": [
    "La diferenciación automática es un método para evaluar derivadas de funciones representadas como programas <a href=\"https://arxiv.org/abs/1502.05767\">[Automatic Differentiation in Machine Learning: a Survey, Baydin et. al, 2018]</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70efdc",
   "metadata": {},
   "source": [
    "<img src=\"difaut1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b13106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "th.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b39d7",
   "metadata": {},
   "source": [
    "## Clase Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fee810",
   "metadata": {},
   "source": [
    "La clase <tt>Parameter</tt> del módulo <tt>nn</tt> de PyTorch define una subclase de <tt>Tensor</tt> que se emplea comúnmente para representar los parámetros que modifican los algoritmos de aprendizaje para generar un modelo. Las instancias de <tt>Parameter</tt> que se definen dentro de una subclase de <tt>Module</tt> del módulo de <tt>nn</tt> se agregan a su lista de parámetros a optimizar.\n",
    "\n",
    "El constructor de la clase <tt>Parameter</tt> recibe un tensor como argumento con el que se crea la instancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09cc0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con un tensor de ceros\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "Con un tensor aleatorio\n",
      "Parameter containing:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
      "        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
      "        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],\n",
      "        [0.4294, 0.8854, 0.5739, 0.2666, 0.6274],\n",
      "        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053],\n",
      "        [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],\n",
      "        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376],\n",
      "        [0.8090, 0.5779, 0.9040, 0.5547, 0.3423],\n",
      "        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890],\n",
      "        [0.2814, 0.7886, 0.5895, 0.7539, 0.1952]], requires_grad=True)\n",
      "Con un arreglo de NumPy\n",
      "Parameter containing:\n",
      "tensor([[0.3745, 0.9507, 0.7320, 0.5987, 0.1560],\n",
      "        [0.1560, 0.0581, 0.8662, 0.6011, 0.7081],\n",
      "        [0.0206, 0.9699, 0.8324, 0.2123, 0.1818],\n",
      "        [0.1834, 0.3042, 0.5248, 0.4319, 0.2912],\n",
      "        [0.6119, 0.1395, 0.2921, 0.3664, 0.4561]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Con una lista\n",
      "Parameter containing:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Con un tensor de ceros')\n",
    "print(nn.parameter.Parameter(th.zeros(10,5)))\n",
    "print('Con un tensor aleatorio')\n",
    "print(nn.parameter.Parameter(th.rand(10,5)))\n",
    "print('Con un arreglo de NumPy')\n",
    "nparray1 = np.random.rand(5,5)\n",
    "print(nn.parameter.Parameter(th.tensor(nparray1)))\n",
    "print('Con una lista')\n",
    "print(nn.parameter.Parameter(th.tensor([[1.,2.],[3.,4.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152f6b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpuesta:\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<PermuteBackward0>)\n",
      "Suma:\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], grad_fn=<AddBackward0>)\n",
      "Multiplicación elemento a elemento:\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "Multiplicación:\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "param = th.nn.parameter.Parameter(th.zeros(10,5))\n",
    "\n",
    "print('Transpuesta:\\n', param.T)\n",
    "print('Suma:\\n', param+th.ones_like(param))\n",
    "print('Multiplicación elemento a elemento:\\n', param*th.zeros_like(param))\n",
    "print('Multiplicación:\\n', param @ th.rand_like(param).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a42622",
   "metadata": {},
   "source": [
    "## Diferenciación automática en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0854a",
   "metadata": {},
   "source": [
    "PyTorch puede diferenciar automáticamente secuencias de operaciones con instancias de <tt>Tensor</tt> o <tt>Parameter</tt>. Para ello se mantiene una gráfica de cómputo, la cual se va generando de manera dinámica conforme se ejecutan operaciones con instancias que tienen la propiedad <tt>requires_grad</tt> en verdadero (solo ten cuidado con las operaciones <i>in-place</i>). Por defecto, todas las instancias de <tt>Parameter</tt> tienen esa propiedad en verdadero y las instancias de <tt>Tensor</tt> en falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03af99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "ten = th.linspace(start=-10, end=10, steps=100)\n",
    "print(param.requires_grad)\n",
    "print(ten.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48311afe",
   "metadata": {},
   "source": [
    "Es posible cambiar el valor de esta propiedad en una instancia usando el método <tt>requires_grad_</tt> (<i>in-place</i>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76e7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "param.requires_grad_(False)\n",
    "ten.requires_grad_(True)\n",
    "print(param.requires_grad)\n",
    "print(ten.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3e496",
   "metadata": {},
   "source": [
    "Para modificar el contenido de una instancia de <tt>Parameter</tt> es necesario especificar que no se registre la operación usando el ámbito <tt>no_grad</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6f23ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo valor del elemento (0,0) = tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    param[0,0] = 1\n",
    "print('Nuevo valor del elemento (0,0) =', param[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b31c6b",
   "metadata": {},
   "source": [
    "En general, existen métodos para instancias tanto de <tt>Parameter</tt> como de <tt>Tensor</tt> que modifican el contenido <i>in-place</i>. Los nombres de estos métodos usualmente terminan con un guión bajo. Tal es el caso de <tt>add_</tt> y <tt>mul_</tt>, que suman y multiplican un tensor. Debido a que estas operaciones no deben registrarse cuando las instancias de <tt>Parameter</tt> o <tt>Tensor</tt> tienen <tt>requires_grad = True</tt>, las ejecutamos dentro del ámbito <tt>no_grad</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d371656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma matriz de unos a parámetros: Parameter containing:\n",
      "tensor([[2., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "Resta matriz de unos a parámetros: Parameter containing:\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "Multiplica parámetros por matriz de dos: Parameter containing:\n",
      "tensor([[2., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    print('Suma matriz de unos a parámetros:', param.add_(th.ones_like(param)))\n",
    "    print('Resta matriz de unos a parámetros:', param.subtract_(th.ones_like(param)))\n",
    "    print('Multiplica parámetros por matriz de dos:', param.mul_(2*th.ones_like(param)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc61f3b",
   "metadata": {},
   "source": [
    "Para obtener el gradiente de una función escalar respecto a un tensor que está siendo contemplado se debe llamar al método <tt>backward</tt>.\n",
    "\n",
    "Por ejemplo, considera la siguiente función: $$ f(x,y) = 2x^3 + 3y^2 + c$$\n",
    "\n",
    "Evaluando esta función en $x=2$, $y=3$ y $c=1.0$, tenemos $$ f(2,3) = 2 \\cdot (2)^3 + 3 \\cdot (3)^2 + 1 = 44$$\n",
    "\n",
    "Las derivadas parciales evaluadas en estos puntos estarían dadas por $$ \\frac{\\partial f}{\\partial x} = 6x^2 = 6 \\cdot (2)^2 = 24$$\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial y} = 6(3)  = 18$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab8ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44., grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "x = th.tensor(2., requires_grad=True)\n",
    "y = th.tensor(3., requires_grad=True)\n",
    "c = th.tensor(1.)\n",
    "f = 2*x**3 + 3*y**2 + c\n",
    "print(f)\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf0e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
